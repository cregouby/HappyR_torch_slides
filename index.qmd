---
title: "{torch} {tabnet} et l'apprentissage profond par l'usage"
subtitle: "Atelier des Rencontres R 2025" 
author: "Christophe Regouby"
footer:  "[{torch}, {tabnet},... par l'usage](https://github.com/cregouby/Tutoriel_torch_slides)"
logo: "images/mlverse.png"
format: 
  revealjs: 
    theme: [night, slides.scss]
    highlight-style: a11y
    transition: fade
    slide-number: true
    chalkboard: true
    lightbox: true
editor: visual
execute:
  freeze: auto
---

# Agenda

Bien commencer avec torch

\{torch\}

Le mlverse

\{tabnet\}

\{tabnet\} pour la regression avec valeurs manquantes

\{tabnet\} pour la classification hierarchique

GPT2 avec R

Fine-Tuning de GPT2 en franÃ§ais avec un LORA

Un classifieur d'images avec ResNext50 fine-tuning

# Bien commencer avec torch

------------------------------------------------------------------------

![](images/clipboard-1733121037.png)

------------------------------------------------------------------------

```{r}
library(countdown)
```

```{r font-awesome-color}
# fill for font awesome icons
fa_fill <- "#C7B41D"
```

## Licence

<br> <br> Ce contenu est sous licence [Creative Commons Attribution-ShareAlike 4.0 International License](https://creativecommons.org/licenses/by-sa/4.0/) (CC BY-SA4.0).

## Checklist

<br>

`r fontawesome::fa("check", fill = fa_fill)` R & RStudio/un IDE confortable installÃ©s?

Â Â Â Â  J'ai R 4.5.0 et RStudio 2025.05.0 build 496

`r fontawesome::fa("check", fill = fa_fill)` {torch} est installÃ© ?

Â Â Â Â  `torch::torch_is_installed()`<br> Â Â Â Â  *Your system is ready!*

`r fontawesome::fa("check", fill = fa_fill)` Un accÃ©lÃ©rateur {torch} ?

Â Â Â Â  `torch::backends_xxxx_is_available()`<br> Â Â Â Â  *Your system has power!*

## Autres ressources {.smaller}

-   torch for R *website* <br> <https://torch.mlverse.org/>

-   Deep-Learning and scientific computing with R *book* <br> <https://skeydan.github.io/Deep-Learning-and-Scientific-Computing-with-R-torch/>

-   torch for R *cheatsheet* <br> <https://rstudio.github.io/cheatsheets/torch.pdf>

-   Deep-learning avec torch *Aide-mÃ©moire* <br> <https://github.com/rstudio/cheatsheets/blob/main/translations/french/torch_fr.pdf>

-   Tutoriel de UseR2021! <https://github.com/mlverse/torch-learnr>

-   Tutoriel des RencontresR 2024 par Tristan Mary-Huard : <https://stateofther.github.io/finistR2022/autodiff.html>

-   le blog AI de Posit (avec la catÃ©gorie torch): <https://blogs.rstudio.com/ai/#category:Torch>

::: footer
:::

# {torch}

## ![](images/clipboard-3520749809.png)![](images/clipboard-4058077362.png)

## ![](images/clipboard-154709511.png)

::: footer
source https://xkcd.com/1987/
:::

## ![](images/clipboard-1267880778.png)

::: footer
source https://xkcd.com/1987/
:::

## [![](images/clipboard-604787592.png)](https://torch.mlverse.org/)

::: footer
:::

## {torch}: pourquoi rÃ©inventer l'eau chaude?

::: small
-   **facilitÃ©** et  **frugalitÃ©**  d'installation sur CPU, GPU, MPS, ...

-   confort de RStudio pour dÃ©velopper, dÃ©verminer, visualiser

-   confort de R pour l'**indexation Ã  1**

-   la qualitÃ© des articles de blog de Posit AI blog

-   l'Ã©cosystÃ¨me des packages

-   plein de possibilitÃ©s de **contributions**

![](images/clipboard-torch0130.png)
![](images/clipboard-torch0142.png)
:::

## Installation

Nominale

![](images/clipboard-install_.png)

AvancÃ©e

![](images/clipboard-install-adv.png)

::: footer
https://torch.mlverse.org/docs/articles/installation.html
:::

------------------------------------------------------------------------

Expert : Machine sans connexion

![](images/clipboard-install-airgap.png)

Expert : dÃ©verminage

```{r}}
Sys.setenv(TORCH_INSTALL_DEBUG = 1)
install_torch()

?install_torch()
```

::: footer
https://torch.mlverse.org/docs/articles/installation.html
:::

## La pile logicielle

![](images/clipboard-1623978672.png){height=60%}

## La manipulation de tenseurs

::::: columns
::: {.column width="50%"}
![](images/clipboard-1496841808.png)
:::

::: {.column width="50%"}
```{r, echo=TRUE}
library(torch)
tt <- torch_rand(2, 3, 4)
tt
```

:::
:::::

------------------------------------------------------------------------

:::::: columns

:::: {.column width="50%"}
::: small

![](images/clipboard-4006123911.png)

```{r, echo=TRUE}
tt[, 2:N, ]
```

:::
::::

:::: {.column width="50%"}
::: small

```{r, echo=TRUE}
tt[1, 2:N, ]
tt[1:1, 2:N, ]
torch_squeeze(tt[1:1, 2:N, ])
```

:::
::::
::::::

------------------------------------------------------------------------

:::::: columns

:::: {.column width="50%"}
![](images/clipboard-torch_device.png)

::: small

```{r, echo=TRUE}
tt$to(device = "cpu")
```

:::
::::

:::: {.column width="50%"}
::: small

```{r, echo=TRUE}
as.array(tt$to(device = "cpu"))
```

:::
::::
::::::

## Ã€ vous de jouer, exercices

Installations : `00_installation.R`

```{r}
countdown(
  minutes = 2,
  # Fanfare when it's over
  play_sound = TRUE,
  # Set timer theme to match solarized colors
  color_border              = "#FFFFFF",
  color_text                = "#7aa81e",
  color_running_background  = "#7aa81e",
  color_running_text        = "#FFFFFF",
  color_finished_background = "#ffa07a",
  color_finished_text       = "#FFFFFF",
  font_size = "2em",
  )
```

------------------------------------------------------------------------

Exercice : `01_exercice.`

```{r}
countdown(
  minutes = 5,
  # Fanfare when it's over
  play_sound = TRUE,
  # Set timer theme to match solarized colors
  color_border              = "#FFFFFF",
  color_text                = "#7aa81e",
  color_running_background  = "#7aa81e",
  color_running_text        = "#FFFFFF",
  color_finished_background = "#ffa07a",
  color_finished_text       = "#FFFFFF",
  font_size = "2em",
  )
```

## les `nn_modules`: construire un rÃ©seau

::: small

```{r, echo=TRUE}
net <- nn_module(
  "Net",
  initialize = function() {
    self$conv1 <- nn_conv2d(1, 32, 3, 1)
    self$conv2 <- nn_conv2d(32, 64, 3, 1)
    self$dropout1 <- nn_dropout(0.25)
    self$dropout2 <- nn_dropout(0.5)
    self$fc1 <- nn_linear(9216, 128)
    self$fc2 <- nn_linear(128, 10)
  },
  forward = function(x) {
    x %>% 
      self$conv1() %>% 
      nnf_relu() %>% 
      self$conv2() %>% 
      nnf_relu() %>% 
      nnf_max_pool2d(2) %>% 
      self$dropout1() %>% 
      torch_flatten(start_dim = 2) %>% 
      self$fc1() %>% 
      nnf_relu() %>% 
      self$dropout2() %>% 
      self$fc2()
  }
)
```

:::


## entraÃ®ner un rÃ©seau avec luz

::: small

```{r, eval=FALSE}
fitted <- net %>%
  setup(
    loss = nn_cross_entropy_loss(),
    optimizer = optim_adam,
    metrics = list(
      luz_metric_accuracy()
    )
  ) %>%
  fit(train_dl, epochs = 10, valid_data = test_dl)
```

:::


# mlverse

## Un univers de ðŸ“¦ dÃ©diÃ©s Ã  {torch}

![Packages du mlverse / utilisant torch](images/tile_mlverse.png)

## Un univers de ðŸ“¦ en franÃ§ais

::: small
| paquetage | messages | l'aide[^1] | les vignettes |
|------------------|------------------|-------------------|------------------|
| {torch} | `r fontawesome::fa("check", fill = fa_fill)` |  |  |
| {torchvision} | `r fontawesome::fa("check", fill = fa_fill)` | `r fontawesome::fa("check", fill = fa_fill)` [/torchvision.fr](https://github.com/cregouby/torchvision.fr) |  |
| {tabnet} | `r fontawesome::fa("check", fill = fa_fill)` | `r fontawesome::fa("check", fill = fa_fill)` [/tabnet.fr](https://github.com/cregouby/tabnet.fr) |  |
| {luz} | `r fontawesome::fa("check", fill = fa_fill)` | `r fontawesome::fa("check", fill = fa_fill)` [/luz.fr](https://github.com/cregouby/luz.fr) | `r fontawesome::fa("check", fill = fa_fill)` [cregouby.github.io/luz.fr/](https://cregouby.github.io/luz.fr/)|
| {hfhub} | `r fontawesome::fa("check", fill = fa_fill)` | `r fontawesome::fa("check", fill = fa_fill)` [/hfhub.fr](https://github.com/cregouby/hfhub.fr) |  |
| {tok} | `r fontawesome::fa("check", fill = fa_fill)` | `r fontawesome::fa("check", fill = fa_fill)` [/tok.fr](https://github.com/cregouby/tok.fr) |  |
| {safetensors} | `r fontawesome::fa("check", fill = fa_fill)` | `r fontawesome::fa("check", fill = fa_fill)` [/safetensors.fr](https://github.com/cregouby/safetensors.fr) |  |
| {minhub} |  | `r fontawesome::fa("check", fill = fa_fill)` [/minhub.fr](https://github.com/cregouby/minhub.fr) |  |

: Les paquetages disponibles en franÃ§ais
:::

[^1]: https://github.com/cregouby/...


------------------------------------------------------------------------

```{r, eval=FALSE, echo=TRUE}
Sys.setLanguage(lang = "fr")
library(torchvision)
transform_normalize(torch::torch_rand(c(3,5,5)), 3, 0)
```

![](images/clipboard-3331382707.png)

------------------------------------------------------------------------

::::: columns
::: {.column width="40%"}

```{r, echo=TRUE}
Sys.setenv(LANGUAGE="fr")
library(torchvision.fr)
library(torchvision)
?transform_normalize
```

:::

::: {.column width="60%"}
![](images/clipboard-1958985727.png)
:::
:::::

# {tabnet}

## ![](images/clipboard-534052795.png)

v0.7.0 is on CRAN

## Fonctionnement

![](images/clipboard-2272997678.png)

## Usage intÃ©grÃ© dans tidymodels {.scrollable .smaller}

::: small
Dataset

```{r}
#| label: "dataset"
#| echo: true
library(tidymodels, quietly = TRUE)
data("ames", package = "modeldata")
str(ames)
```

Recipe

```{r}
#| label: "tabnet recipe"
#| echo: true
ames <- ames |> mutate(Sale_Price = log10(Sale_Price))
ames_rec <- recipe(Sale_Price ~ ., data=ames) |> 
  step_normalize(all_numeric(), -all_outcomes()) 
```

:::

::: footer
:::

------------------------------------------------------------------------

::: small
Pre-training

```{r}
#| label: "tabnet pretrain"
#| cache: true
#| echo: true
library(tabnet)
ames_pretrain <- tabnet_pretrain(
  ames_rec, data=ames,  epoch=50, cat_emb_dim = 1,
  valid_split = 0.2, verbose=TRUE, 
  early_stopping_patience = 3L, 
  early_stopping_tolerance = 1e-4
)
# model diagnostic
autoplot(ames_pretrain)
```

:::

------------------------------------------------------------------------

::: small
Training

```{r}
#| label: "tabnet fit"
#| cache: true
#| echo: true
ames_fit <- tabnet_fit(ames_rec, data=ames,  tabnet_model = ames_pretrain, 
                       epoch=50, cat_emb_dim = 1, 
                       valid_split = 0.2, verbose=TRUE, batch=2930, 
                       early_stopping_patience = 5L, 
                       early_stopping_tolerance = 1e-4)
# model diagnostic
autoplot(ames_fit)
```

:::

------------------------------------------------------------------------

:::::: small

Prediction

::::: columns
::: {.column width="30%"}
```{r}
#| label: "tabnet predict"
#| echo: true
predict(ames_fit, ames)
```

:::

::: {.column width="70%"}
```{r}
#| echo: true
metrics <- metric_set(rmse, rsq, ccc)
cbind(ames, predict(ames_fit, ames)) |> 
  metrics(Sale_Price, estimate = .pred)
```

:::
:::::
::::::

---

::: small

```{r}
#| label: "tabnet vip"
#| echo: true
# variable importance
vip::vip(ames_fit)
```

:::

------------------------------------------------------------------------

::: small

Explainability

```{r}
#| label: "tabnet explain"
#| echo: true
ames_explain <- tabnet::tabnet_explain(ames_fit, ames)
# variable importance
autoplot(ames_explain, quantile = 0.99)
```

:::

## Ã€ vous de jouer, exercise 02

::: question
Complete 02_exercise to practice tabnet model training.
:::

```{r}
countdown(
  minutes = 7
  )
```

::: footer
:::

# {tabnet} pour les valeurs manquantes

## retour sur le jeu de donnÃ©es Ames

:::: small

-   les tenseurs ne peuvent pas inclure de valeur manquantes.

-   `ames` nous fait le plaisir d'Ãªtre sans valeur manquante.

::: question
Quelle est la surface de la piscine quand il n'y a pas de piscine?
:::

::::

------------------------------------------------------------------------

```{r, echo=TRUE}
data("ames", package = "modeldata")
qplot(ames$Mas_Vnr_Area)
```

::: question
Comment le modÃ¨le peut-il capturer cette distribution ?
:::

------------------------------------------------------------------------

Et si on l'applique Ã  toute les colonnes ?

::: small

```{r}
#| echo: TRUE
#| code-fold: TRUE
#| label: "tabnet missing dataset"
col_with_zero_as_na <- ames |>  
  select(where(is.numeric)) |>  
  select(matches("_SF|Area|Misc_Val|[Pp]orch$")) |> 
  summarise_each(min) |> 
  select_if(~.x==0) |> 
  names()
ames_missing <- ames |>mutate_at(col_with_zero_as_na, na_if, 0) |> 
  mutate_at("Alley", na_if, "No_Alley_Access")  |>  
  mutate_at("Fence", na_if, "No_Fence") |> 
  mutate_at(c("Garage_Cond", "Garage_Finish"), na_if, "No_Garage") |> 
  mutate_at(c("Bsmt_Exposure", "BsmtFin_Type_1", "BsmtFin_Type_2"), na_if, "No_Basement")

visdat::vis_miss(ames_missing)
```

:::

------------------------------------------------------------------------

::: small

Recipe

```{r}
#| label: "tabnet missing recipe"
#| echo: true
ames_missing <- ames_missing |> mutate(Sale_Price = log10(Sale_Price))
ames_missing_rec <- recipe(Sale_Price ~ ., data=ames_missing) |> 
  step_normalize(all_numeric(), -all_outcomes()) 
```

Pre-training

```{r}
#| label: "tabnet missing pretrain"
#| echo: true
library(tabnet)
ames_missing_pretrain <- tabnet_pretrain(
  ames_missing_rec, data=ames_missing,  epoch=50, cat_emb_dim = 1,  valid_split = 0.2,
  verbose=TRUE,   early_stopping_patience = 3L,   early_stopping_tolerance = 1e-4
)
# model diagnostic
autoplot(ames_missing_pretrain)
```

:::

------------------------------------------------------------------------

::: small

Training

```{r}
#| label: "tabnet missing fit"
#| cache: true
#| echo: true
#| code-fold: TRUE

ames_missing_fit <- tabnet_fit(
  ames_missing_rec,   data = ames_missing,
  tabnet_model = ames_missing_pretrain,
  epoch = 50,  cat_emb_dim = 1,  valid_split = 0.2,
  verbose = TRUE,  batch = 2930,
  early_stopping_patience = 5L,
  early_stopping_tolerance = 1e-4
)
# model diagnostic
autoplot(ames_missing_fit)
```

:::

------------------------------------------------------------------------

:::::: small

Prediction

::::: columns
::: {.column width="30%"}
```{r}
#| label: "tabnet missing predict"
#| echo: true
predict(ames_missing_fit, ames_missing)
```

:::

::: {.column width="70%"}
```{r}
#| echo: true
metrics <- metric_set(rmse, rsq, ccc)
cbind(ames_missing, predict(ames_missing_fit, ames_missing)) |> 
  metrics(Sale_Price, estimate = .pred)
```

:::
:::::
::::::

------------------------------------------------------------------------

:::::: small

Variable importance

::::: columns
::: {.column width="50%"}


```{r}
#| code-fold: TRUE
col_with_missings <- ames_missing |>
  summarise_all(~sum(is.na(.))>0) |>
  t() |> enframe(name="Variable") |>
  rename(has_missing="value")

vip_color <- function(object, col_has_missing) {
  vip_data <- vip::vip(object)$data |> arrange(Importance)
  vis_miss_plus <- left_join(vip_data, col_has_missing , by="Variable") |>
    mutate(Variable=factor(Variable, levels = vip_data$Variable))
  vis_miss_plus
  ggplot(vis_miss_plus, aes(x=Variable, y=Importance, fill=has_missing)) +
    geom_col() + coord_flip() + scale_fill_grey()
}
```


```{r}
#| label: "tabnet pretrain vip color"
#| echo: TRUE
# original ames
vip_color(ames_pretrain, col_with_missings)
vip_color(ames_fit, col_with_missings)
```

:::

::: {.column width="50%"}

```{r}
#| label: "tabnet pretrain missing vip color"
#| echo: true
# ames with missing values
vip_color(ames_missing_pretrain, col_with_missings)
vip_color(ames_missing_fit, col_with_missings)
```

:::
:::::
::::::

------------------------------------------------------------------------

::: small

Explainability

```{r}
#| label: "tabnet missing explain"
#| echo: true
ames_missing_explain <- tabnet::tabnet_explain(ames_missing_fit, ames_missing)
# variable importance
autoplot(ames_missing_explain, quantile = 0.99, type="step")
```

:::

# {tabnet} avec un `outcome()` hierarchique


------------------------------------------------------------------------

* {tabnet} admet des variable Ã  prÃ©dire catÃ©gorielle, multi-label multi-class.
* et si on pouvait mettre une contrainte entre les classes des diffÃ©rents labels ?

* le dataset doit Ãªtre de type `data.tree::as.Node()`
  - conversion de trainset et testset avec `as.Node()` avant les fonctions `tabnet_`
  - conversion de inverse avec  `node_to_df()`

* nouveautÃ© de la 0.5.0

## Exemple avec `starwars`

::: small

```{r}
#| echo: true

library(data.tree)
data(starwars, package = "dplyr")
head(starwars, 4)
```

:::

---

::: small

On construit la variable de sortie comme un chaÃ®ne avec des sÃ©parateurs `/` dans une variable `"pathString"` (erronnÃ©)

```{r}
#| echo: true
starwars_tree <- starwars |> 
  mutate(pathString = paste("StarWars_characters", species, sex, `name`, sep = "/"))  |> 
  as.Node()
print(starwars_tree, "name","height", "mass", "eye_color", limit = 8)
```

:::

## Mais avec des rÃªgles sur les noms et les types 

::: small
- pas d'usage des noms internes de {data.tree} : 
  - `name`, `height` sont interdits 
  - comme tous les noms de `NODE_RESERVED_NAMES_CONST`.
  (Ils seraient supprimÃ©s au moment de la conversion.)
- pas de `factor()`
- pas de colonne nomÃ©e `level_*`
- le dernier niveau hiÃ©rarchique doit Ãªtre l'individu (donc un Id unique)
- il doit y avoir une racine Ã  la hiÃ©rarchie
:::


## Construction correcte de la variable de sortie `"pathString"`

::: small

```{r}
#| echo: true
starwars_tree <- starwars |>
  rename(`_name` = "name", `_height` = "height") |> 
  mutate(pathString = paste("StarWars_characters", species, sex, `_name`, sep = "/"))  |> 
  as.Node()
print(starwars_tree, "name", "_name","_height", "mass", "eye_color", limit = 8)
```

:::
 
## Initial split et construction

`starwars` a des colonnes de `list()` qu'il faut dÃ©rouler

::: small

```{r}
#| echo: true
starw_split <- starwars |> 
  tidyr::unnest_longer(films) |> 
  tidyr::unnest_longer(vehicles, keep_empty = TRUE) |> 
  tidyr::unnest_longer(starships, keep_empty = TRUE) |> 
  initial_split( prop = .8, strata = "species")
```

:::

--- 

::: small

```{r}
#| echo: true
starwars_train_tree <- starw_split |> 
  training() |>
  rename(`_name` = "name", `_height` = "height") |>
  rowid_to_column() |>
  mutate(pathString = paste("StarWars_characters", species, sex, rowid, sep = "/")) |>
  # remove outcomes labels from predictors
  select(-species, -sex, -`_name`, -rowid) |>
  # turn it as hierarchical Node
  as.Node()

starwars_test_tree <- starw_split |>
  testing() |>
  rename(`_name` = "name", `_height` = "height") |>
  rowid_to_column() |>
  mutate(pathString = paste("StarWars_characters", species, sex, rowid, sep = "/")) |>
  select(-species, -sex, -`_name`, -rowid) |>
  as.Node()
```

Les `$attributesAll` du Node seront les predicteurs : 
```{r}
#| echo: true
starwars_train_tree$attributesAll
```

:::

## EntraÃ®nement du modÃ¨le
```{{r}}
#| echo: true
#| label: "starwars fit"

config <- tabnet_config(
  decision_width = 8,
  attention_width = 8,
  num_steps = 3,
  penalty = .003,
  cat_emb_dim = 2,
  valid_split = 0.2,
  learn_rate = 1e-3,
  lr_scheduler = "reduce_on_plateau",
  early_stopping_monitor = "valid_loss",
  early_stopping_patience = 4,
  verbose = FALSE
)

starw_model <- tabnet_fit(starwars_train_tree, config = config, epoch = 75, checkpoint_epochs = 15)
```

## Diagnostique
```{{r}}
#| echo: true
#| label: "starwars diag"

autoplot(starw_model)
```

```{{r}}
#| echo: true
#| label: "starwars vip"

vip::vip(starw_model)
```

## InfÃ©rence sur le modÃ¨le hierarchique
```{{r}}
#| echo: true
#| label: "starwars inference"
starwars_hat <- bind_cols(
    predict(starw_model, starwars_test_tree),
    node_to_df(starwars_test_tree)$y
  )
tail(starwars_hat, n = 5)
```

# GPT2 avec R

basÃ© sur 4 packages {minhub}, {hfhub}, {tok}, {safetensors}

-   {minhub} : un dÃ©pot de **rÃ©seau de neurones** classiques pour {torch}
-   {hfhub} : l'accÃ¨s aux tÃ©lÃ©chargement de **modÃ¨les** prÃ©entraÃ®nÃ©s du hub hugging-face
-   {tok} : un wrappeur des tokenizers d'huggingface en R
-   {safetensors} : sauvegarde et lecture des donnÃ©es de tenseurs au format `.safetensors`

## TÃ©lÃ©chargement du modÃ¨le et de ses poids

```{r}
#| echo: true
#| label: "GPT2 model download"
library(minhub)
library(zeallot)

identifier <- "gpt2"
revision <- "e7da7f2"
# instantiate model and load Hugging Face weights
model <- gpt2_from_pretrained(identifier, revision)
# load matching tokenizer
tok <- tok::tokenizer$from_pretrained(identifier,)
model$eval()
```

## Tokenisation de la phrase

```{r}
#| echo: true
#| label: "GPT2 tokenize"

text = paste("âœ¨ Quel plaisir de participer aux Rencontres R 2025 Ã  Mons !âœ¨",
             "Vivement la prochaine" )

idx <- torch_tensor(tok$encode(text)$ids)$view(c(1, -1))
idx
```

## GÃ©nÃ©ration d'une entrÃ©e

La gÃ©nÃ©ration est un process itÃ©ratif, chaque prÃ©diction du modÃ¨le est ajoutÃ©e au prompt qui grossit.

Ajoutons y 30 tokens :

```{r}
#| echo: true
#| label: "GPT2 generate"
prompt_length <- idx$size(-1)

for (i in 1:30) { # decide on maximal length of output sequence
  # obtain next prediction (raw score)
  with_no_grad({
    logits <- model(idx + 1L)
  })
  last_logits <- logits[, -1, ]
  # pick highest scores (how many is up to you)
  c(prob, ind) %<-% last_logits$topk(50)
  last_logits <- torch_full_like(last_logits, -Inf)$scatter_(-1, ind, prob)
  # convert to probabilities
  probs <- nnf_softmax(last_logits, dim = -1)
  # probabilistic sampling
  id_next <- torch_multinomial(probs, num_samples = 1) - 1L
  # stop if end of sequence predicted
  if (id_next$item() == 0) {
    break
  }
  # append prediction to prompt
  idx <- torch_cat(list(idx, id_next), dim = 2)
}
```

## dÃ©codage des tokens du rÃ©sultat

```{r}
#| echo: true
#| label: "GPT2 decode"
tok$decode(as.integer(idx))
```

# Fine-Tuning avec LoRA

## Est-ce que les LLMs dÃ©possÃ¨dent le data-scientist ?

-   des rÃ©seaux toujours plus gros impliquent des entraÃ®nements prohibitifs

-   la promesse de la prochaÃ®ne version qui rÃ©soudra les faiblesses constatÃ©es

-   le jeu de donnÃ©e de rÃ©ference difficile Ã  constituer

## LoRA Ã  la rescousse

Low Rank Adaptation

![](images/lora.png)

## Method

::: small
The problem of fine-tuning a neural network can be expressed by finding a $\Delta \Theta$ that minimizes $L(X, y; \Theta_0 + \Delta\Theta)$ where $L$ is a loss function, $X$ and $y$ are the data and $\Theta_0$ the weights from a pre-trained model.

We learn the parameters $\Delta \Theta$ with dimension $|\Delta \Theta|$ equals to $|\Theta_0|$. When $|\Theta_0|$ is very large, such as in large scale pre-trained models, finding $\Delta \Theta$ becomes computationally challenging. Also, for each task you need to learn a new $\Delta \Theta$ parameter set, making it even more challenging to deploy fine-tuned models if you have more than a few specific tasks.
LoRA proposes using an approximation $\Delta \Phi \approx \Delta \Theta$ with $|\Delta \Phi| << |\Delta \Theta|$. The observation is that neural nets have many dense layers performing matrix multiplication, and while they typically have full-rank during pre-training, when adapting to a specific task the weight updates will have a low "intrinsic dimension".

:::

::: footer
d'aprÃ¨s https://blogs.rstudio.com/ai/posts/2023-06-22-understanding-lora/
:::
------------------------------------------------------------------------

::: small

A simple matrix decomposition is applied for each weight matrix update $\Delta \theta \in \Delta \Theta$. Considering $\Delta \theta_i \in \mathbb{R}^{d \times k}$ the update for the $i$th weight in the network, LoRA approximates it with:

$$\Delta \theta_i  \approx \Delta \phi_i = BA$$ where $B \in \mathbb{R}^{d \times r}$, $A \in \mathbb{R}^{r \times d}$ and the rank $r << min(d, k)$. Thus instead of learning $d \times k$ parameters we now need to learn $(d + k) \times r$ which is easily a lot smaller given the multiplicative aspect. In practice, $\Delta \theta_i$ is scaled by $\frac{\alpha}{r}$ before being added to $\theta_i$, which can be interpreted as a 'learning rate' for the LoRA update.

LoRA does not increase inference latency, as once fine tuning is done, you can simply update the weights in $\Theta$ by adding their respective $\Delta \theta \approx \Delta \phi$. It also makes it simpler to deploy multiple task specific models on top of one large model, as $|\Delta \Phi|$ is much smaller than $|\Delta \Theta|$.

:::

::: footer
d'aprÃ¨s https://blogs.rstudio.com/ai/posts/2023-06-22-understanding-lora/
:::

## ImplÃ©mentation avec torch

On simule un jeu de donnÃ©es $y = X \theta$ model. $\theta \in \mathbb{R}^{1001, 1000}$.

```{r}
#| echo: true
#| label: "LoRA"
library(torch)

n <- 10000
d_in <- 1001
d_out <- 1000

thetas <- torch_randn(d_in, d_out)

X <- torch_randn(n, d_in)
y <- torch_matmul(X, thetas)
```

------------------------------------------------------------------------

On entraine un modÃ¨le pour estimer $\theta$. C'est notre modÃ¨le entraÃ®nÃ©.

```{r}
#| echo: true
#| code-fold: true
#| label: "pre-LoRA single layer network"
lin_model <- nn_linear(d_in, d_out, bias = FALSE)


```

```{r}
#| echo: true
#| code-fold: true
#| label: "pre-LoRA model"
train <- function(model, X, y, batch_size = 128, epochs = 100) {
  opt <- optim_adam(model$parameters)

  for (epoch in 1:epochs) {
    for(i in seq_len(n/batch_size)) {
      idx <- sample.int(n, size = batch_size)
      loss <- nnf_mse_loss(model(X[idx,]), y[idx])
      
      with_no_grad({
        opt$zero_grad()
        loss$backward()
        opt$step()  
      })
    }
    
    if (epoch %% 10 == 0) {
      with_no_grad({
        loss <- nnf_mse_loss(model(X), y)
      })
      cat("[", epoch, "] Loss:", loss$item(), "\n")
    }
  }
}
```

------------------------------------------------------------------------

On entraÃ®ne le modÃ¨le   

```{r}
#| echo: true
#| code-fold: true
#| label: "pre-LoRA train"
train(lin_model, X, y)
```

------------------------------------------------------------------------

On simule une distribution des donnÃ©es diffÃ©rente en appliquant une transformation Ã  $\theta$

```{r}
#| echo: true
#| code-fold: true
#| label: "LoRA shifted density data"
thetas2 <- thetas + 1

X2 <- torch_randn(n, d_in)
y2 <- torch_matmul(X2, thetas2)
```

Sur ces donnÃ©es, le modÃ¨le donne de mauvais rÃ©sultats : 
```{r}
#| echo: true
nnf_mse_loss(lin_model(X2), y2)
```

---

::: small

Le nouveau LoRA 
 - s'ajoute au modÃ¨le `linear` dont on gÃ¨le les poids
 - avec des tenseurs A et B de dimension intÃ©rieure $r$
 
```{r}
#| echo: true
#| label: "LoRA add weight to model"
lora_nn_linear <- nn_module(
  initialize = function(linear, r = 16L, alpha = 1) {
    self$linear <- linear
    
    # parameters from the original linear module are 'freezed', so they are not
    # tracked by autograd. They are considered just constants.
    purrr::walk(self$linear$parameters, \(x) x$requires_grad_(FALSE))
    
    # the low rank parameters that will be trained (shortcut here, specific to our lin_model)
    self$A <- nn_parameter(torch_randn(linear$in_features, r))
    self$B <- nn_parameter(torch_zeros(r, linear$out_features))
    
    # the scaling constant
    self$scaling <- alpha / r
  },
  forward = function(x) {
    # the modified forward, that just adds the result from the base model
    # and ABx.
    self$linear(x) + torch_matmul(x, torch_matmul(self$A, self$B) * self$scaling)
  }
)
```

:::

---

Essayons un LoRA avec $r = 1$ i.e. A et B sont des vecteurs

```{r}
#| echo: true
#| label: "LoRA-added linear model"

lora <- lora_nn_linear(lin_model, r = 1L)
```

entraÃ®nement du LoRA sur la nouvelle distribution

```{r}
#| echo: true
#| cache: true
#| label: "train LoRA"
train(lora, X2, y2)
```

---

Le tenseur $\Delta \theta$ est idÃ©alement constant Ã  1 :
```{r}
#| echo: true
#| label: "LoRA check conformity"
delta_theta <- torch_matmul(lora$A, lora$B)*lora$scaling
delta_theta[1:5, 1:5]

```

---

::: small

Pour diminuer le temps d'infÃ©rence, une astuce consiste Ã  ajouter le LoRA directement au poids du modÃ¨le avec la fonction `$add_`. Ainsi on passe de deux infÃ©rences sÃ©quentielle, Ã  une seule.
```{r}
#| echo: true
with_no_grad({
  lin_model$weight$add_(delta_theta$t())  
})
```

Quel est la performance sur la nouvelle distribution ? 
```{r}
#| echo: true
nnf_mse_loss(lin_model(X2), y2)

```

:::



# Un classifieur d'images mÃ©tier avec le fine tuning de ResNext50

```{r}
#| label: TBC
library(minhub)
library(hfhub)
withr::with_envvar(new = c("SSL_CERT_FILE"="/Users/to94498/.ssl/airbus-ca.crt"),
      yolo_8n_nn <- hub_download("nielsr/yolov8n", filename = "model.safetensors")
      )
#> Error: Timeout was reached [huggingface.co]: Resolving timed out after 10001 milliseconds
```

