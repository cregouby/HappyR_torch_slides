---
title: "{torch} {tabnet} et l'apprentissage profond par l'usage"
subtitle: "Atelier des Rencontres R 2025" 
author: "Christophe Regouby"
footer:  "[{torch}, {tabnet},... par l'usage](https://github.com/cregouby/Tutoriel_torch_slides)"
logo: "images/mlverse.png"
format: 
  revealjs: 
    theme: [night, slides.scss]
    highlight-style: a11y
    transition: fade
    slide-number: true
    chalkboard: true
    lightbox: true
    code-line-numbers: false
editor: visual
execute:
  freeze: auto
---

# Agenda {.smaller}

Bien commencer avec torch

\{torch\}

Le mlverse

\{tabnet\}

-  \{tabnet\} pour la rÃ©gression avec valeurs manquantes
  
-  \{tabnet\} pour la classification hiÃ©rarchique
  
-  \{tabnet\} pour la rÃ©gression logistique dÃ©sÃ©quilibrÃ©e

GPT2 avec R

Fine-Tuning de GPT2 en franÃ§ais avec un LORA

Un classifieur d'images avec ResNext50 fine-tuning

# Bien commencer avec torch

------------------------------------------------------------------------

![](images/clipboard-1733121037.png)

------------------------------------------------------------------------

```{r}
library(countdown)
```

```{r font-awesome-color}
# fill for font awesome icons
fa_fill <- "#C7B41D"
```

## Licence

<br> <br> Ce contenu est sous licence [Creative Commons Attribution-ShareAlike 4.0 International License](https://creativecommons.org/licenses/by-sa/4.0/) (CC BY-SA4.0).

## Checklist

<br>

`r fontawesome::fa("check", fill = fa_fill)` R & RStudio/un IDE confortable installÃ©s?

Â Â Â Â  J'ai R 4.5.0 et RStudio 2025.05.0 build 496

`r fontawesome::fa("check", fill = fa_fill)` {torch} est installÃ© ?

Â Â Â Â  `torch::torch_is_installed()`<br> Â Â Â Â  *Your system is ready!*

`r fontawesome::fa("check", fill = fa_fill)` Un accÃ©lÃ©rateur {torch} ?

Â Â Â Â  `torch::backends_xxxx_is_available()`<br> Â Â Â Â  *Your system has power!*

## Autres ressources {.smaller}

-   torch for R *website* <br> <https://torch.mlverse.org/>

-   Deep-Learning and scientific computing with R *book* <br> <https://skeydan.github.io/Deep-Learning-and-Scientific-Computing-with-R-torch/>

-   torch for R *cheatsheet* <br> <https://rstudio.github.io/cheatsheets/torch.pdf>

-   Deep-learning avec torch *Aide-mÃ©moire* <br> <https://github.com/rstudio/cheatsheets/blob/main/translations/french/torch_fr.pdf>

-   Tutoriel de UseR2021! <https://github.com/mlverse/torch-learnr>

-   Tutoriel des RencontresR 2024 par Tristan Mary-Huard : <https://stateofther.github.io/finistR2022/autodiff.html>

-   le blog AI de Posit (avec la catÃ©gorie torch): <https://blogs.rstudio.com/ai/#category:Torch>

::: footer
:::

# {torch}

## ![](images/clipboard-3520749809.png)![](images/clipboard-4058077362.png)

## ![](images/clipboard-154709511.png)

::: footer
source https://xkcd.com/1987/
:::

## ![](images/clipboard-1267880778.png)

::: footer
source https://xkcd.com/1987/
:::

## [![](images/clipboard-604787592.png)](https://torch.mlverse.org/)

::: footer
:::

## {torch}: pourquoi rÃ©inventer l'eau chaude?

::: small
-   **facilitÃ©** et  **frugalitÃ©**  d'installation sur CPU, GPU, MPS, ...

-   confort de RStudio pour dÃ©velopper, dÃ©verminer, visualiser

-   confort de R pour l'**indexation Ã  1**

-   la qualitÃ© des articles de blog de Posit AI blog

-   l'Ã©cosystÃ¨me des packages

-   plein de possibilitÃ©s de **contributions**

![](images/clipboard-torch0130.png)
![](images/clipboard-torch0142.png)
:::

## Installation

Nominale

![](images/clipboard-install_.png)

AvancÃ©e

![](images/clipboard-install-adv.png)

::: footer
https://torch.mlverse.org/docs/articles/installation.html
:::

------------------------------------------------------------------------

Expert : Machine sans connexion

![](images/clipboard-install-airgap.png)

Expert : dÃ©verminage

```{r}}
Sys.setenv(TORCH_INSTALL_DEBUG = 1)
install_torch()

?install_torch()
```

::: footer
https://torch.mlverse.org/docs/articles/installation.html
:::

## La pile logicielle

![](images/clipboard-1623978672.png){height=60%}

## La manipulation de tenseurs

::::: columns
::: {.column width="50%"}
![](images/clipboard-1496841808.png)
:::

::: {.column width="50%"}
```{r, echo=TRUE}
library(torch)
tt <- torch_rand(2, 3, 4)
tt
```

:::
:::::

------------------------------------------------------------------------

:::::: columns

:::: {.column width="50%"}
::: small

![](images/clipboard-4006123911.png)

```{r, echo=TRUE}
tt[, 2:N, ]
```

:::
::::

:::: {.column width="50%"}
::: small

```{r, echo=TRUE}
tt[1, 2:N, ]
tt[1:1, 2:N, ]
torch_squeeze(tt[1:1, 2:N, ])
```

:::
::::
::::::

------------------------------------------------------------------------

:::::: columns

:::: {.column width="50%"}
![](images/clipboard-torch_device.png)

::: small

```{r, echo=TRUE}
tt$to(device = "cpu")
```

:::
::::

:::: {.column width="50%"}
::: small

```{r, echo=TRUE}
as.array(tt$to(device = "cpu"))
```

:::
::::
::::::

## Ã€ vous de jouer, exercices

Installations : `00_installation.R`

```{r}
countdown(
  minutes = 2,
  # Fanfare when it's over
  play_sound = TRUE,
  # Set timer theme to match solarized colors
  color_border              = "#FFFFFF",
  color_text                = "#7aa81e",
  color_running_background  = "#7aa81e",
  color_running_text        = "#FFFFFF",
  color_finished_background = "#ffa07a",
  color_finished_text       = "#FFFFFF",
  font_size = "2em",
  )
```

------------------------------------------------------------------------

Exercice : `01_exercice.`

```{r}
countdown(
  minutes = 5,
  # Fanfare when it's over
  play_sound = TRUE,
  # Set timer theme to match solarized colors
  color_border              = "#FFFFFF",
  color_text                = "#7aa81e",
  color_running_background  = "#7aa81e",
  color_running_text        = "#FFFFFF",
  color_finished_background = "#ffa07a",
  color_finished_text       = "#FFFFFF",
  font_size = "2em",
  )
```

## les `nn_modules`: construire un rÃ©seau

::: small

```{r, echo=TRUE}
net <- nn_module(
  "Net",
  initialize = function(num_class) {
    self$conv1 <- nn_conv2d(1, 32, 3, 1)
    self$conv2 <- nn_conv2d(32, 64, 3, 1)
    self$dropout1 <- nn_dropout(0.25)
    self$dropout2 <- nn_dropout(0.5)
    self$fc1 <- nn_linear(9216, 128)
    self$fc2 <- nn_linear(128, num_class)
  },
  forward = function(x) {
    x %>% 
      self$conv1() %>% 
      nnf_relu() %>% 
      self$conv2() %>% 
      nnf_relu() %>% 
      nnf_max_pool2d(2) %>% 
      self$dropout1() %>% 
      torch_flatten(start_dim = 2) %>% 
      self$fc1() %>% 
      nnf_relu() %>% 
      self$dropout2() %>% 
      self$fc2()
  }
)
```

:::


## entraÃ®ner un rÃ©seau avec luz {.small}

```{r}
#| eval: false
#| echo: true
fitted <- net %>%
  setup(
    loss = nn_cross_entropy_loss(),
    optimizer = optim_ignite_adam,
    metrics = list(
      luz_metric_accuracy()
    )
  ) %>%
  set_hparams(num_class = 10) %>% 
  set_opt_hparams(lr = 0.003) %>%
  fit(train_dl, epochs = 10, valid_data = test_dl)
```

------------------------------------------------------------------------

Exercice : `02_exercice.`

```{r}
countdown(
  minutes = 5,
  # Fanfare when it's over
  play_sound = TRUE,
  # Set timer theme to match solarized colors
  color_border              = "#FFFFFF",
  color_text                = "#7aa81e",
  color_running_background  = "#7aa81e",
  color_running_text        = "#FFFFFF",
  color_finished_background = "#ffa07a",
  color_finished_text       = "#FFFFFF",
  font_size = "2em",
  )
```

# mlverse

## Un univers de ðŸ“¦ dÃ©diÃ©s Ã  {torch}

![Packages du mlverse / utilisant torch](images/tile_mlverse.png)

## Un univers de ðŸ“¦ en franÃ§ais

::: small
| paquetage | messages | l'aide[^1] | les vignettes |
|------------------|------------------|-------------------|------------------|
| {torch} | `r fontawesome::fa("check", fill = fa_fill)` |  |  |
| {torchvision} | `r fontawesome::fa("check", fill = fa_fill)` | `r fontawesome::fa("check", fill = fa_fill)` [/torchvision.fr](https://github.com/cregouby/torchvision.fr) |  |
| {tabnet} | `r fontawesome::fa("check", fill = fa_fill)` | `r fontawesome::fa("check", fill = fa_fill)` [/tabnet.fr](https://github.com/cregouby/tabnet.fr) |  |
| {luz} | `r fontawesome::fa("check", fill = fa_fill)` | `r fontawesome::fa("check", fill = fa_fill)` [/luz.fr](https://github.com/cregouby/luz.fr) | `r fontawesome::fa("check", fill = fa_fill)` [cregouby.github.io/luz.fr/](https://cregouby.github.io/luz.fr/)|
| {hfhub} | `r fontawesome::fa("check", fill = fa_fill)` | `r fontawesome::fa("check", fill = fa_fill)` [/hfhub.fr](https://github.com/cregouby/hfhub.fr) |  |
| {tok} | `r fontawesome::fa("check", fill = fa_fill)` | `r fontawesome::fa("check", fill = fa_fill)` [/tok.fr](https://github.com/cregouby/tok.fr) |  |
| {safetensors} | `r fontawesome::fa("check", fill = fa_fill)` | `r fontawesome::fa("check", fill = fa_fill)` [/safetensors.fr](https://github.com/cregouby/safetensors.fr) |  |
| {minhub} | `r fontawesome::fa("check", fill = fa_fill)`[^2] | `r fontawesome::fa("check", fill = fa_fill)` [/minhub.fr](https://github.com/cregouby/minhub.fr) |  |

: Les paquetages disponibles en franÃ§ais

[^1]: https://github.com/cregouby/
[^2]: P.R. en cours

:::

------------------------------------------------------------------------

```{r, eval=FALSE, echo=TRUE}
Sys.setLanguage(lang = "fr")
library(torchvision)
transform_normalize(torch::torch_rand(c(3,5,5)), 3, 0)
```

![](images/clipboard-3331382707.png)

------------------------------------------------------------------------

::::: columns
::: {.column width="40%"}

```{r, echo=TRUE}
Sys.setenv(LANGUAGE="fr")
library(torchvision.fr)
library(torchvision)
?transform_normalize
```

:::

::: {.column width="60%"}
![](images/clipboard-1958985727.png)
:::
:::::
{{< include _tabnet.qmd >}}

# GPT2 avec R

basÃ© sur 4 packages {minhub}, {hfhub}, {tok}, {safetensors}

-   {minhub} : un dÃ©pot de **rÃ©seau de neurones** classiques pour {torch}
-   {hfhub} : l'accÃ¨s aux tÃ©lÃ©chargement de **modÃ¨les** prÃ©entraÃ®nÃ©s du hub hugging-face
-   {tok} : un wrappeur des tokenizers d'huggingface en R
-   {safetensors} : sauvegarde et lecture des donnÃ©es de tenseurs au format `.safetensors`

## TÃ©lÃ©chargement du modÃ¨le et de ses poids

```{r}
#| echo: true
#| label: "GPT2 model download"
library(minhub)
library(zeallot)

identifier <- "gpt2"
revision <- "e7da7f2"
# instantiate model and load Hugging Face weights
model <- gpt2_from_pretrained(identifier, revision)
# load matching tokenizer
tok <- tok::tokenizer$from_pretrained(identifier,)
model$eval()
```

## Tokenisation de la phrase

```{r}
#| echo: true
#| label: "GPT2 tokenize"

text = paste("âœ¨ Quel plaisir de participer aux Rencontres R 2025 Ã  Mons !âœ¨",
             "Vivement la prochaine" )

idx <- torch_tensor(tok$encode(text)$ids)$view(c(1, -1))
idx
```

## GÃ©nÃ©ration d'une entrÃ©e

La gÃ©nÃ©ration est un process itÃ©ratif, chaque prÃ©diction du modÃ¨le est ajoutÃ©e au prompt qui grossit.

Ajoutons y 30 tokens :

```{r}
#| echo: true
#| label: "GPT2 generate"
prompt_length <- idx$size(-1)

for (i in 1:30) { # decide on maximal length of output sequence
  # obtain next prediction (raw score)
  with_no_grad({
    logits <- model(idx + 1L)
  })
  last_logits <- logits[, -1, ]
  # pick highest scores (how many is up to you)
  c(prob, ind) %<-% last_logits$topk(50)
  last_logits <- torch_full_like(last_logits, -Inf)$scatter_(-1, ind, prob)
  # convert to probabilities
  probs <- nnf_softmax(last_logits, dim = -1)
  # probabilistic sampling
  id_next <- torch_multinomial(probs, num_samples = 1) - 1L
  # stop if end of sequence predicted
  if (id_next$item() == 0) {
    break
  }
  # append prediction to prompt
  idx <- torch_cat(list(idx, id_next), dim = 2)
}
```

## dÃ©codage des tokens du rÃ©sultat

```{r}
#| echo: true
#| label: "GPT2 decode"
tok$decode(as.integer(idx))
```

# Fine-Tuning avec LoRA

## Est-ce que les LLMs dÃ©possÃ¨dent le data-scientist ?

-   des rÃ©seaux toujours plus gros impliquent des entraÃ®nements prohibitifs

-   la promesse de la prochaÃ®ne version qui rÃ©soudra les faiblesses constatÃ©es

-   le jeu de donnÃ©e de rÃ©ference difficile Ã  constituer

## LoRA Ã  la rescousse

Low Rank Adaptation

![](images/lora.png)

## Method

::: small
The problem of fine-tuning a neural network can be expressed by finding a $\Delta \Theta$ that minimizes $L(X, y; \Theta_0 + \Delta\Theta)$ where $L$ is a loss function, $X$ and $y$ are the data and $\Theta_0$ the weights from a pre-trained model.

We learn the parameters $\Delta \Theta$ with dimension $|\Delta \Theta|$ equals to $|\Theta_0|$. When $|\Theta_0|$ is very large, such as in large scale pre-trained models, finding $\Delta \Theta$ becomes computationally challenging. Also, for each task you need to learn a new $\Delta \Theta$ parameter set, making it even more challenging to deploy fine-tuned models if you have more than a few specific tasks.
LoRA proposes using an approximation $\Delta \Phi \approx \Delta \Theta$ with $|\Delta \Phi| << |\Delta \Theta|$. The observation is that neural nets have many dense layers performing matrix multiplication, and while they typically have full-rank during pre-training, when adapting to a specific task the weight updates will have a low "intrinsic dimension".

:::

::: footer
d'aprÃ¨s https://blogs.rstudio.com/ai/posts/2023-06-22-understanding-lora/
:::
------------------------------------------------------------------------

::: small

A simple matrix decomposition is applied for each weight matrix update $\Delta \theta \in \Delta \Theta$. Considering $\Delta \theta_i \in \mathbb{R}^{d \times k}$ the update for the $i$th weight in the network, LoRA approximates it with:

$$\Delta \theta_i  \approx \Delta \phi_i = BA$$ where $B \in \mathbb{R}^{d \times r}$, $A \in \mathbb{R}^{r \times d}$ and the rank $r << min(d, k)$. Thus instead of learning $d \times k$ parameters we now need to learn $(d + k) \times r$ which is easily a lot smaller given the multiplicative aspect. In practice, $\Delta \theta_i$ is scaled by $\frac{\alpha}{r}$ before being added to $\theta_i$, which can be interpreted as a 'learning rate' for the LoRA update.

LoRA does not increase inference latency, as once fine tuning is done, you can simply update the weights in $\Theta$ by adding their respective $\Delta \theta \approx \Delta \phi$. It also makes it simpler to deploy multiple task specific models on top of one large model, as $|\Delta \Phi|$ is much smaller than $|\Delta \Theta|$.

:::

::: footer
d'aprÃ¨s https://blogs.rstudio.com/ai/posts/2023-06-22-understanding-lora/
:::

## ImplÃ©mentation avec torch

On simule un jeu de donnÃ©es $y = X \theta$ model. $\theta \in \mathbb{R}^{1001, 1000}$.

```{r}
#| echo: true
#| label: "LoRA"
library(torch)

n <- 10000
d_in <- 1001
d_out <- 1000

thetas <- torch_randn(d_in, d_out)

X <- torch_randn(n, d_in)
y <- torch_matmul(X, thetas)
```

------------------------------------------------------------------------

On entraine un modÃ¨le pour estimer $\theta$. C'est notre modÃ¨le entraÃ®nÃ©.

```{r}
#| echo: true
#| code-fold: true
#| label: "pre-LoRA single layer network"
lin_model <- nn_linear(d_in, d_out, bias = FALSE)


```

```{r}
#| echo: true
#| code-fold: true
#| label: "pre-LoRA model"
train <- function(model, X, y, batch_size = 128, epochs = 100) {
  opt <- optim_ignite_adam(model$parameters)

  for (epoch in 1:epochs) {
    for(i in seq_len(n/batch_size)) {
      idx <- sample.int(n, size = batch_size)
      loss <- nnf_mse_loss(model(X[idx,]), y[idx])
      
      with_no_grad({
        opt$zero_grad()
        loss$backward()
        opt$step()  
      })
    }
    
    if (epoch %% 10 == 0) {
      with_no_grad({
        loss <- nnf_mse_loss(model(X), y)
      })
      cat("[", epoch, "] Loss:", loss$item(), "\n")
    }
  }
}
```

------------------------------------------------------------------------

On entraÃ®ne le modÃ¨le   

```{r}
#| echo: true
#| code-fold: true
#| label: "pre-LoRA train"
train(lin_model, X, y)
```

------------------------------------------------------------------------

On simule une distribution des donnÃ©es diffÃ©rente en appliquant une transformation Ã  $\theta$

```{r}
#| echo: true
#| code-fold: true
#| label: "LoRA shifted density data"
thetas2 <- thetas + 1

X2 <- torch_randn(n, d_in)
y2 <- torch_matmul(X2, thetas2)
```

Sur ces donnÃ©es, le modÃ¨le donne de mauvais rÃ©sultats : 
```{r}
#| echo: true
nnf_mse_loss(lin_model(X2), y2)
```

---

::: small

Le nouveau LoRA 
 - s'ajoute au modÃ¨le `linear` dont on gÃ¨le les poids
 - avec des tenseurs A et B de dimension intÃ©rieure $r$
 
```{r}
#| echo: true
#| label: "LoRA add weight to model"
lora_nn_linear <- nn_module(
  initialize = function(linear, r = 16L, alpha = 1) {
    self$linear <- linear
    
    # les paramÃªtres du modÃ¨le linÃ©irte initial sont  'gelÃ©s', donc pas pris en 
    # considÃ©ration pr autograd. Ce sont juste des constantes.
    purrr::walk(self$linear$parameters, \(x) x$requires_grad_(FALSE))
    
    # LEs paramÃªtre du Low-rank Ã  entraÃ®ner (shortcut here, specific to our lin_model)
    self$A <- nn_parameter(torch_randn(linear$in_features, r))
    self$B <- nn_parameter(torch_zeros(r, linear$out_features))
    
    # la constante de scaling
    self$scaling <- alpha / r
  },
  forward = function(x) {
    # La fonction `forward` modifiÃ©e qui ajoute le rÃ©sultat du LoRA  A.B.x aux rÃ©sultat du modÃ¨le initial
    self$linear(x) + torch_matmul(x, torch_matmul(self$A, self$B) * self$scaling)
  }
)
```

:::

---

Essayons un LoRA avec $r = 1$ i.e. A et B sont des vecteurs

```{r}
#| echo: true
#| label: "LoRA-added linear model"

lora <- lora_nn_linear(lin_model, r = 1L)
```

entraÃ®nement du LoRA sur la nouvelle distribution

```{r}
#| echo: true
#| cache: true
#| label: "train LoRA"
train(lora, X2, y2)
```

---

Le tenseur $\Delta \theta$ est idÃ©alement constant Ã  1 :
```{r}
#| echo: true
#| label: "LoRA check conformity"
delta_theta <- torch_matmul(lora$A, lora$B) * lora$scaling
```
```{r}
#| echo: true
#| eval: false
delta_theta[1:5, 1:5]
#> torch_tensor
#> 1.0005  1.0005  1.0005  1.0005  1.0005
#> 1.0000  1.0000  1.0000  1.0000  1.0000
#> 0.9958  0.9958  0.9958  0.9958  0.9958
#> 0.9993  0.9993  0.9993  0.9993  0.9993
#> 0.9997  0.9997  0.9997  0.9997  0.9997
#>[ CPUFloatType{5,5} ][ grad_fn = <SliceBackward0> ]
```

---

::: {.smaller .scrollable}

Pour diminuer le temps d'infÃ©rence, une astuce consiste Ã  ajouter le LoRA directement au poids du modÃ¨le avec la fonction `$add_`. Ainsi on passe de deux infÃ©rences sÃ©quentielle, Ã  une seule.
```{r}
#| echo: true
with_no_grad({
  lin_model$weight$add_(delta_theta$t())  
})
```

:::

---

Quel est la performance sur la nouvelle distribution ? 
```{r}
#| echo: true
#| eval: false
nnf_mse_loss(lin_model(X2), y2)
#> torch_tensor
#> 0.0012366
#> [ CPUFloatType{} ]
```


{{< include _image_classifier.qmd >}}
