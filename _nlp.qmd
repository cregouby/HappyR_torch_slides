---
title: "{torch} {tabnet} et l'apprentissage profond par l'usage"
format: 
  revealjs: 
    theme: [night, slides.scss]
    lightbox: true
editor: visual
---

# GPT2 avec R

basé sur 4 packages {minhub}, {hfhub}, {tok}, {safetensors}

-   {minhub} : un dépot de **réseau de neurones** classiques pour {torch}
-   {hfhub} : l'accès aux téléchargement de **modèles** préentraînés du hub hugging-face
-   {tok} : un wrappeur des tokenizers d'huggingface en R
-   {safetensors} : sauvegarde et lecture des données de tenseurs au format `.safetensors`

## Téléchargement du modèle et de ses poids

```{r}
#| echo: true
#| label: "GPT2 model download"
library(minhub)
library(zeallot)

identifier <- "gpt2"
revision <- "e7da7f2"
# instantiate model and load Hugging Face weights
model <- gpt2_from_pretrained(identifier, revision)
# load matching tokenizer
tok <- tok::tokenizer$from_pretrained(identifier,)
model$eval()
```

## Tokenisation de la phrase

```{r}
#| echo: true
#| label: "GPT2 tokenize"

text = paste("✨ Quel plaisir de participer aux  ateliers de R-toulouse !✨",
             "Vivement la proch" )

idx <- torch_tensor(tok$encode(text)$ids)$view(c(1, -1))
idx
```

## Génération d'une entrée

La génération est un process itératif, chaque prédiction du modèle est ajoutée au prompt qui grossit.

Ajoutons y 30 tokens :

```{r}
#| echo: true
#| label: "GPT2 generate"
prompt_length <- idx$size(-1)

for (i in 1:30) { # decide on maximal length of output sequence
  # obtain next prediction (raw score)
  with_no_grad({
    logits <- model(idx + 1L)
  })
  last_logits <- logits[, -1, ]
  # pick highest scores (how many is up to you)
  c(prob, ind) %<-% last_logits$topk(50)
  last_logits <- torch_full_like(last_logits, -Inf)$scatter_(-1, ind, prob)
  # convert to probabilities
  probs <- nnf_softmax(last_logits, dim = -1)
  # probabilistic sampling
  id_next <- torch_multinomial(probs, num_samples = 1) - 1L
  # stop if end of sequence predicted
  if (id_next$item() == 0) {
    break
  }
  # append prediction to prompt
  idx <- torch_cat(list(idx, id_next), dim = 2)
}
```

## décodage des tokens du résultat

```{r}
#| echo: true
#| label: "GPT2 decode"
tok$decode(as.integer(idx))
```

# Fine-Tuning avec LoRA

## Est-ce que les LLMs dépossèdent le data-scientist ?

-   des réseaux toujours plus gros impliquent des entraînements prohibitifs

-   la promesse de la prochaîne version qui résoudra les faiblesses constatées

-   le jeu de donnée de réference difficile à constituer

## LoRA à la rescousse

Low Rank Adaptation

![](images/lora.png)

## Method

::: small
The problem of fine-tuning a neural network can be expressed by finding a $\Delta \Theta$ that minimizes $L(X, y; \Theta_0 + \Delta\Theta)$ where $L$ is a loss function, $X$ and $y$ are the data and $\Theta_0$ the weights from a pre-trained model.

We learn the parameters $\Delta \Theta$ with dimension $|\Delta \Theta|$ equals to $|\Theta_0|$. When $|\Theta_0|$ is very large, such as in large scale pre-trained models, finding $\Delta \Theta$ becomes computationally challenging. Also, for each task you need to learn a new $\Delta \Theta$ parameter set, making it even more challenging to deploy fine-tuned models if you have more than a few specific tasks.
LoRA proposes using an approximation $\Delta \Phi \approx \Delta \Theta$ with $|\Delta \Phi| << |\Delta \Theta|$. The observation is that neural nets have many dense layers performing matrix multiplication, and while they typically have full-rank during pre-training, when adapting to a specific task the weight updates will have a low "intrinsic dimension".

:::

::: footer
d'après https://blogs.rstudio.com/ai/posts/2023-06-22-understanding-lora/
:::
------------------------------------------------------------------------

::: small

A simple matrix decomposition is applied for each weight matrix update $\Delta \theta \in \Delta \Theta$. Considering $\Delta \theta_i \in \mathbb{R}^{d \times k}$ the update for the $i$th weight in the network, LoRA approximates it with:

$$\Delta \theta_i  \approx \Delta \phi_i = BA$$ where $B \in \mathbb{R}^{d \times r}$, $A \in \mathbb{R}^{r \times d}$ and the rank $r << min(d, k)$. Thus instead of learning $d \times k$ parameters we now need to learn $(d + k) \times r$ which is easily a lot smaller given the multiplicative aspect. In practice, $\Delta \theta_i$ is scaled by $\frac{\alpha}{r}$ before being added to $\theta_i$, which can be interpreted as a 'learning rate' for the LoRA update.

LoRA does not increase inference latency, as once fine tuning is done, you can simply update the weights in $\Theta$ by adding their respective $\Delta \theta \approx \Delta \phi$. It also makes it simpler to deploy multiple task specific models on top of one large model, as $|\Delta \Phi|$ is much smaller than $|\Delta \Theta|$.

:::

::: footer
d'après https://blogs.rstudio.com/ai/posts/2023-06-22-understanding-lora/
:::

## Implémentation avec torch

On simule un jeu de données $y = X \theta$ model. $\theta \in \mathbb{R}^{1001, 1000}$.

```{r}
#| echo: true
#| label: "LoRA"
library(torch)

n <- 10000
d_in <- 1001
d_out <- 1000

thetas <- torch_randn(d_in, d_out)

X <- torch_randn(n, d_in)
y <- torch_matmul(X, thetas)
```

------------------------------------------------------------------------

On entraine un modèle pour estimer $\theta$. C'est notre modèle entraîné.

```{r}
#| echo: true
#| code-fold: true
#| label: "pre-LoRA single layer network"
lin_model <- nn_linear(d_in, d_out, bias = FALSE)


```

```{r}
#| echo: true
#| code-fold: true
#| label: "pre-LoRA model"
train <- function(model, X, y, batch_size = 128, epochs = 100) {
  opt <- optim_ignite_adam(model$parameters)

  for (epoch in 1:epochs) {
    for(i in seq_len(n/batch_size)) {
      idx <- sample.int(n, size = batch_size)
      loss <- nnf_mse_loss(model(X[idx,]), y[idx])
      
      with_no_grad({
        opt$zero_grad()
        loss$backward()
        opt$step()  
      })
    }
    
    if (epoch %% 10 == 0) {
      with_no_grad({
        loss <- nnf_mse_loss(model(X), y)
      })
      cat("[", epoch, "] Loss:", loss$item(), "\n")
    }
  }
}
```

------------------------------------------------------------------------

On entraîne le modèle   

```{r}
#| echo: true
#| code-fold: true
#| label: "pre-LoRA train"
train(lin_model, X, y)
```

------------------------------------------------------------------------

On simule une distribution des données différente en appliquant une transformation à $\theta$

```{r}
#| echo: true
#| code-fold: true
#| label: "LoRA shifted density data"
thetas2 <- thetas + 1

X2 <- torch_randn(n, d_in)
y2 <- torch_matmul(X2, thetas2)
```

Sur ces données, le modèle donne de mauvais résultats : 
```{r}
#| echo: true
nnf_mse_loss(lin_model(X2), y2)
```

---

::: small

Le nouveau LoRA 
 - s'ajoute au modèle `linear` dont on gèle les poids
 - avec des tenseurs A et B de dimension intérieure $r$
 
```{r}
#| echo: true
#| label: "LoRA add weight to model"
lora_nn_linear <- nn_module(
  initialize = function(linear, r = 16L, alpha = 1) {
    self$linear <- linear
    
    # les paramêtres du modèle linéirte initial sont  'gelés', donc pas pris en 
    # considération pr autograd. Ce sont juste des constantes.
    purrr::walk(self$linear$parameters, \(x) x$requires_grad_(FALSE))
    
    # LEs paramêtre du Low-rank à entraîner (shortcut here, specific to our lin_model)
    self$A <- nn_parameter(torch_randn(linear$in_features, r))
    self$B <- nn_parameter(torch_zeros(r, linear$out_features))
    
    # la constante de scaling
    self$scaling <- alpha / r
  },
  forward = function(x) {
    # La fonction `forward` modifiée qui ajoute le résultat du LoRA  A.B.x aux résultat du modèle initial
    self$linear(x) + torch_matmul(x, torch_matmul(self$A, self$B) * self$scaling)
  }
)
```

:::

---

Essayons un LoRA avec $r = 1$ i.e. A et B sont des vecteurs

```{r}
#| echo: true
#| label: "LoRA-added linear model"

lora <- lora_nn_linear(lin_model, r = 1L)
```

entraînement du LoRA sur la nouvelle distribution

```{r}
#| echo: true
#| cache: true
#| label: "train LoRA"
train(lora, X2, y2)
```

---

Le tenseur $\Delta \theta$ est idéalement constant à 1 :
```{r}
#| echo: true
#| label: "LoRA check conformity"
delta_theta <- torch_matmul(lora$A, lora$B) * lora$scaling
```
```{r}
#| echo: true
#| eval: false
delta_theta[1:5, 1:5]
#> torch_tensor
#> 1.0005  1.0005  1.0005  1.0005  1.0005
#> 1.0000  1.0000  1.0000  1.0000  1.0000
#> 0.9958  0.9958  0.9958  0.9958  0.9958
#> 0.9993  0.9993  0.9993  0.9993  0.9993
#> 0.9997  0.9997  0.9997  0.9997  0.9997
#>[ CPUFloatType{5,5} ][ grad_fn = <SliceBackward0> ]
```

---

::: {.smaller .scrollable}

Pour diminuer le temps d'inférence, une astuce consiste à ajouter le LoRA directement au poids du modèle avec la fonction `$add_`. Ainsi on passe de deux inférences séquentielle, à une seule.
```{r}
#| echo: true
with_no_grad({
  lin_model$weight$add_(delta_theta$t())  
})
```

:::

---

Quel est la performance sur la nouvelle distribution ? 
```{r}
#| echo: true
#| eval: false
nnf_mse_loss(lin_model(X2), y2)
#> torch_tensor
#> 0.0012366
#> [ CPUFloatType{} ]
```

